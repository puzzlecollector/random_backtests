{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import random \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import time \n",
    "import datetime \n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor \n",
    "import json \n",
    "import ccxt \n",
    "from datetime import datetime, timedelta \n",
    "import seaborn as sns \n",
    "import pandas_ta as ta \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78331/78331 [00:00<00:00, 437975.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>바이든, 마이클 바 연준 부의장 지명... 전 리플 이사회 멤버</td>\n",
       "      <td>코인데스크에 따르면 바이든 미국 대통령이 미국 재무부 소속 공무원이자 전 리플(XR...</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스위스 취리히 공항에 대형 비트코인 광고판 등장</td>\n",
       "      <td>암호화폐 전문 미디어 비트코인매거진에 따르면, 스위스 취리히 공항에 대형 비트코인 ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BTC, 지난 5분간 1.18% 상승</td>\n",
       "      <td>BTC가 바이낸스 USDT 마켓 기준 지난 5분간 1.18% 상승했다. 현재 BTC...</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  바이든, 마이클 바 연준 부의장 지명... 전 리플 이사회 멤버   \n",
       "1           스위스 취리히 공항에 대형 비트코인 광고판 등장   \n",
       "2                 BTC, 지난 5분간 1.18% 상승   \n",
       "\n",
       "                                             content  year  month  day  hour  \n",
       "0  코인데스크에 따르면 바이든 미국 대통령이 미국 재무부 소속 공무원이자 전 리플(XR...  2022      4   16     0  \n",
       "1  암호화폐 전문 미디어 비트코인매거진에 따르면, 스위스 취리히 공항에 대형 비트코인 ...  2022      4   16     0  \n",
       "2  BTC가 바이낸스 USDT 마켓 기준 지난 5분간 1.18% 상승했다. 현재 BTC...  2022      4   15    23  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tokenpost_속보.csv\") \n",
    "train_news = pd.read_csv(\"most_recent_btcusdt/train_finetune.csv\")\n",
    "test_news = pd.read_csv(\"most_recent_btcusdt/test_finetune.csv\") \n",
    "\n",
    "def add_datetime(df):\n",
    "    dates = df['dates'].values \n",
    "    years, months, days, hours = [], [], [], [] \n",
    "    for i in tqdm(range(len(dates)), position=0, leave=True):\n",
    "        splitted = dates[i].split(' ') \n",
    "        d1 = splitted[0].split('-') \n",
    "        years.append(int(d1[0])) \n",
    "        months.append(int(d1[1])) \n",
    "        days.append(int(d1[2])) \n",
    "        t = splitted[2].split(\":\") \n",
    "        hours.append(int(t[0]))  \n",
    "    df['year'] = years \n",
    "    df['month'] = months \n",
    "    df['day'] = days \n",
    "    df['hour'] = hours \n",
    "    df.rename(columns={'titles':'title', 'contents':'content'},inplace=True)\n",
    "    return df \n",
    "\n",
    "df = add_datetime(df)\n",
    "df.drop(columns={'dates'},inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a581366291a64267b6cdd121f07be4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/398 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674e974609af44d09df770af19af8337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2283fc375b144f148d7dab5c01d5493c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/245 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c41c60a4a54e219be4c4ba3983aba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/667 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e06fd4852f4ba3bcbc0cca30160a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CryptoModel/cryptobert-base-all were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CryptoModel/cryptobert-base-all and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CryptoModel/cryptobert-base-all\", use_auth_token=\"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\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"CryptoModel/cryptobert-base-all\", use_auth_token=\"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\", num_labels=3)\n",
    "\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(s1,s2,MAX_LEN=512): \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = s1, \n",
    "        text_pair = s2, \n",
    "        add_special_tokens = True, \n",
    "        pad_to_max_length = True, \n",
    "        max_length = MAX_LEN, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    token_type_id = encoded_dict['token_type_ids'] \n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1,s2 = df['title'].values[10], df['content'].values[10]\n",
    "\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.to(device)\n",
    "inputs = tokenizer(s1,s2, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>82.088865</td>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4333.32</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4333.32</td>\n",
       "      <td>4427.30</td>\n",
       "      <td>63.619882</td>\n",
       "      <td>2017-08-17 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4436.06</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4333.42</td>\n",
       "      <td>4352.34</td>\n",
       "      <td>174.562001</td>\n",
       "      <td>2017-08-17 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4352.33</td>\n",
       "      <td>4354.84</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4325.23</td>\n",
       "      <td>225.109716</td>\n",
       "      <td>2017-08-17 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4307.56</td>\n",
       "      <td>4369.69</td>\n",
       "      <td>4258.56</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>249.769913</td>\n",
       "      <td>2017-08-17 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10239</th>\n",
       "      <td>40585.54</td>\n",
       "      <td>40640.00</td>\n",
       "      <td>39320.02</td>\n",
       "      <td>39433.60</td>\n",
       "      <td>13727.261710</td>\n",
       "      <td>2022-04-22 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10240</th>\n",
       "      <td>39433.60</td>\n",
       "      <td>39786.62</td>\n",
       "      <td>39177.00</td>\n",
       "      <td>39502.92</td>\n",
       "      <td>9189.603560</td>\n",
       "      <td>2022-04-22 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241</th>\n",
       "      <td>39502.92</td>\n",
       "      <td>39835.23</td>\n",
       "      <td>39401.00</td>\n",
       "      <td>39709.18</td>\n",
       "      <td>3716.510950</td>\n",
       "      <td>2022-04-22 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10242</th>\n",
       "      <td>39709.19</td>\n",
       "      <td>39872.64</td>\n",
       "      <td>39406.25</td>\n",
       "      <td>39463.02</td>\n",
       "      <td>3424.421740</td>\n",
       "      <td>2022-04-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>39463.01</td>\n",
       "      <td>39687.77</td>\n",
       "      <td>39285.00</td>\n",
       "      <td>39600.98</td>\n",
       "      <td>3627.328930</td>\n",
       "      <td>2022-04-23 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           open      high       low     close        volume  \\\n",
       "0       4261.48   4349.99   4261.32   4349.99     82.088865   \n",
       "1       4333.32   4485.39   4333.32   4427.30     63.619882   \n",
       "2       4436.06   4485.39   4333.42   4352.34    174.562001   \n",
       "3       4352.33   4354.84   4200.74   4325.23    225.109716   \n",
       "4       4307.56   4369.69   4258.56   4285.08    249.769913   \n",
       "...         ...       ...       ...       ...           ...   \n",
       "10239  40585.54  40640.00  39320.02  39433.60  13727.261710   \n",
       "10240  39433.60  39786.62  39177.00  39502.92   9189.603560   \n",
       "10241  39502.92  39835.23  39401.00  39709.18   3716.510950   \n",
       "10242  39709.19  39872.64  39406.25  39463.02   3424.421740   \n",
       "10243  39463.01  39687.77  39285.00  39600.98   3627.328930   \n",
       "\n",
       "                  datetime  \n",
       "0      2017-08-17 04:00:00  \n",
       "1      2017-08-17 08:00:00  \n",
       "2      2017-08-17 12:00:00  \n",
       "3      2017-08-17 16:00:00  \n",
       "4      2017-08-17 20:00:00  \n",
       "...                    ...  \n",
       "10239  2022-04-22 12:00:00  \n",
       "10240  2022-04-22 16:00:00  \n",
       "10241  2022-04-22 20:00:00  \n",
       "10242  2022-04-23 00:00:00  \n",
       "10243  2022-04-23 04:00:00  \n",
       "\n",
       "[10244 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataframe \n",
    "import json\n",
    "\n",
    "with open('BTC_USDT-4h-3.json') as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "chart_df = pd.DataFrame(d)\n",
    "chart_df = chart_df.rename(columns={0:\"timestamp\",\n",
    "                                    1:\"open\",\n",
    "                                    2:\"high\",\n",
    "                                    3:\"low\",\n",
    "                                    4:\"close\",\n",
    "                                    5:\"volume\"})\n",
    "\n",
    "def process(df): \n",
    "    binance = ccxt.binance() \n",
    "    dates = df['timestamp'].values \n",
    "    timestamp = [] \n",
    "    for i in range(len(dates)): \n",
    "        date_string = binance.iso8601(int(dates[i])) \n",
    "        date_string = date_string[:10] + \" \" + date_string[11:-5] \n",
    "        timestamp.append(date_string) \n",
    "    df['datetime'] = timestamp \n",
    "    df = df.drop(columns={'timestamp'})\n",
    "    return df\n",
    "\n",
    "chart_df = process(chart_df)\n",
    "\n",
    "chart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-17 04:00:00</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>82.088865</td>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 08:00:00</th>\n",
       "      <td>4333.32</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4333.32</td>\n",
       "      <td>4427.30</td>\n",
       "      <td>63.619882</td>\n",
       "      <td>2017-08-17 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 12:00:00</th>\n",
       "      <td>4436.06</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4333.42</td>\n",
       "      <td>4352.34</td>\n",
       "      <td>174.562001</td>\n",
       "      <td>2017-08-17 12:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 16:00:00</th>\n",
       "      <td>4352.33</td>\n",
       "      <td>4354.84</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4325.23</td>\n",
       "      <td>225.109716</td>\n",
       "      <td>2017-08-17 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 20:00:00</th>\n",
       "      <td>4307.56</td>\n",
       "      <td>4369.69</td>\n",
       "      <td>4258.56</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>249.769913</td>\n",
       "      <td>2017-08-17 20:00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-22 12:00:00</th>\n",
       "      <td>40585.54</td>\n",
       "      <td>40640.00</td>\n",
       "      <td>39320.02</td>\n",
       "      <td>39433.60</td>\n",
       "      <td>13727.261710</td>\n",
       "      <td>2022-04-22 12:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-22 16:00:00</th>\n",
       "      <td>39433.60</td>\n",
       "      <td>39786.62</td>\n",
       "      <td>39177.00</td>\n",
       "      <td>39502.92</td>\n",
       "      <td>9189.603560</td>\n",
       "      <td>2022-04-22 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-22 20:00:00</th>\n",
       "      <td>39502.92</td>\n",
       "      <td>39835.23</td>\n",
       "      <td>39401.00</td>\n",
       "      <td>39709.18</td>\n",
       "      <td>3716.510950</td>\n",
       "      <td>2022-04-22 20:00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-23 00:00:00</th>\n",
       "      <td>39709.19</td>\n",
       "      <td>39872.64</td>\n",
       "      <td>39406.25</td>\n",
       "      <td>39463.02</td>\n",
       "      <td>3424.421740</td>\n",
       "      <td>2022-04-23 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-23 04:00:00</th>\n",
       "      <td>39463.01</td>\n",
       "      <td>39687.77</td>\n",
       "      <td>39285.00</td>\n",
       "      <td>39600.98</td>\n",
       "      <td>3627.328930</td>\n",
       "      <td>2022-04-23 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close        volume  \\\n",
       "datetime                                                                    \n",
       "2017-08-17 04:00:00   4261.48   4349.99   4261.32   4349.99     82.088865   \n",
       "2017-08-17 08:00:00   4333.32   4485.39   4333.32   4427.30     63.619882   \n",
       "2017-08-17 12:00:00   4436.06   4485.39   4333.42   4352.34    174.562001   \n",
       "2017-08-17 16:00:00   4352.33   4354.84   4200.74   4325.23    225.109716   \n",
       "2017-08-17 20:00:00   4307.56   4369.69   4258.56   4285.08    249.769913   \n",
       "...                       ...       ...       ...       ...           ...   \n",
       "2022-04-22 12:00:00  40585.54  40640.00  39320.02  39433.60  13727.261710   \n",
       "2022-04-22 16:00:00  39433.60  39786.62  39177.00  39502.92   9189.603560   \n",
       "2022-04-22 20:00:00  39502.92  39835.23  39401.00  39709.18   3716.510950   \n",
       "2022-04-23 00:00:00  39709.19  39872.64  39406.25  39463.02   3424.421740   \n",
       "2022-04-23 04:00:00  39463.01  39687.77  39285.00  39600.98   3627.328930   \n",
       "\n",
       "                                datetime  hour  day  month  year  Targets  \n",
       "datetime                                                                   \n",
       "2017-08-17 04:00:00  2017-08-17 04:00:00     4   17      8  2017      0.0  \n",
       "2017-08-17 08:00:00  2017-08-17 08:00:00     8   17      8  2017      0.0  \n",
       "2017-08-17 12:00:00  2017-08-17 12:00:00    12   17      8  2017      1.0  \n",
       "2017-08-17 16:00:00  2017-08-17 16:00:00    16   17      8  2017      1.0  \n",
       "2017-08-17 20:00:00  2017-08-17 20:00:00    20   17      8  2017      0.0  \n",
       "...                                  ...   ...  ...    ...   ...      ...  \n",
       "2022-04-22 12:00:00  2022-04-22 12:00:00    12   22      4  2022      1.0  \n",
       "2022-04-22 16:00:00  2022-04-22 16:00:00    16   22      4  2022      0.0  \n",
       "2022-04-22 20:00:00  2022-04-22 20:00:00    20   22      4  2022      0.0  \n",
       "2022-04-23 00:00:00  2022-04-23 00:00:00     0   23      4  2022      1.0  \n",
       "2022-04-23 04:00:00  2022-04-23 04:00:00     4   23      4  2022      NaN  \n",
       "\n",
       "[10244 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10244/10244 [00:04<00:00, 2061.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Engineering ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Targets</th>\n",
       "      <th>ebsw</th>\n",
       "      <th>cmf</th>\n",
       "      <th>vwap/open</th>\n",
       "      <th>high/low</th>\n",
       "      <th>close/open</th>\n",
       "      <th>...</th>\n",
       "      <th>open_change_4</th>\n",
       "      <th>high_change_4</th>\n",
       "      <th>low_change_4</th>\n",
       "      <th>close_change_4</th>\n",
       "      <th>volume_change_4</th>\n",
       "      <th>open_change_5</th>\n",
       "      <th>high_change_5</th>\n",
       "      <th>low_change_5</th>\n",
       "      <th>close_change_5</th>\n",
       "      <th>volume_change_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-23 16:00:00</th>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.121501</td>\n",
       "      <td>0.980956</td>\n",
       "      <td>1.038033</td>\n",
       "      <td>0.974007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.046272</td>\n",
       "      <td>1.031877</td>\n",
       "      <td>1.022327</td>\n",
       "      <td>1.004063</td>\n",
       "      <td>1.093820</td>\n",
       "      <td>1.060846</td>\n",
       "      <td>1.048760</td>\n",
       "      <td>1.040436</td>\n",
       "      <td>1.019077</td>\n",
       "      <td>1.406636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-23 20:00:00</th>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>1.001464</td>\n",
       "      <td>1.026746</td>\n",
       "      <td>0.994568</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012716</td>\n",
       "      <td>1.015253</td>\n",
       "      <td>1.008545</td>\n",
       "      <td>1.009603</td>\n",
       "      <td>1.075979</td>\n",
       "      <td>1.023881</td>\n",
       "      <td>1.012272</td>\n",
       "      <td>1.013929</td>\n",
       "      <td>1.003317</td>\n",
       "      <td>0.955893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hour  day  month  year  Targets     ebsw       cmf  \\\n",
       "datetime                                                                  \n",
       "2017-08-23 16:00:00    16   23      8  2017      1.0  0.00000  0.121501   \n",
       "2017-08-23 20:00:00    20   23      8  2017      0.0  0.57735  0.097590   \n",
       "\n",
       "                     vwap/open  high/low  close/open  ...  open_change_4  \\\n",
       "datetime                                              ...                  \n",
       "2017-08-23 16:00:00   0.980956  1.038033    0.974007  ...       1.046272   \n",
       "2017-08-23 20:00:00   1.001464  1.026746    0.994568  ...       1.012716   \n",
       "\n",
       "                     high_change_4  low_change_4  close_change_4  \\\n",
       "datetime                                                           \n",
       "2017-08-23 16:00:00       1.031877      1.022327        1.004063   \n",
       "2017-08-23 20:00:00       1.015253      1.008545        1.009603   \n",
       "\n",
       "                     volume_change_4  open_change_5  high_change_5  \\\n",
       "datetime                                                             \n",
       "2017-08-23 16:00:00         1.093820       1.060846       1.048760   \n",
       "2017-08-23 20:00:00         1.075979       1.023881       1.012272   \n",
       "\n",
       "                     low_change_5  close_change_5  volume_change_5  \n",
       "datetime                                                            \n",
       "2017-08-23 16:00:00      1.040436        1.019077         1.406636  \n",
       "2017-08-23 20:00:00      1.013929        1.003317         0.955893  \n",
       "\n",
       "[2 rows x 37 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataframe \n",
    "import json\n",
    "\n",
    "with open('BTC_USDT-4h-3.json') as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "chart_df = pd.DataFrame(d)\n",
    "chart_df = chart_df.rename(columns={0:\"timestamp\",\n",
    "                                    1:\"open\",\n",
    "                                    2:\"high\",\n",
    "                                    3:\"low\",\n",
    "                                    4:\"close\",\n",
    "                                    5:\"volume\"})\n",
    "\n",
    "def process(df): \n",
    "    binance = ccxt.binance() \n",
    "    dates = df['timestamp'].values \n",
    "    timestamp = [] \n",
    "    for i in range(len(dates)): \n",
    "        date_string = binance.iso8601(int(dates[i])) \n",
    "        date_string = date_string[:10] + \" \" + date_string[11:-5] \n",
    "        timestamp.append(date_string) \n",
    "    df['datetime'] = timestamp \n",
    "    df = df.drop(columns={'timestamp'})\n",
    "    return df\n",
    "\n",
    "chart_df = process(chart_df)\n",
    "\n",
    "hours = []\n",
    "days = [] \n",
    "months = [] \n",
    "years = [] \n",
    "for dt in tqdm(chart_df['datetime']):\n",
    "    hour = pd.to_datetime(dt).hour \n",
    "    day = pd.to_datetime(dt).day \n",
    "    month = pd.to_datetime(dt).month \n",
    "    year = pd.to_datetime(dt).year \n",
    "    hours.append(hour) \n",
    "    days.append(day) \n",
    "    months.append(month)\n",
    "    years.append(year) \n",
    "\n",
    "chart_df['hour'] = hours\n",
    "chart_df['day'] = days \n",
    "chart_df['month'] = months \n",
    "chart_df['year'] = years \n",
    "\n",
    "targets = [] \n",
    "close = chart_df['open'].values \n",
    "high = chart_df['high'].values \n",
    "low = chart_df['low'].values \n",
    "\n",
    "threshold = 0.0075\n",
    "\n",
    "for i in range(close.shape[0]-1):\n",
    "    high_volatility = (high[i+1]-close[i]) / close[i] \n",
    "    low_volatility = (low[i+1]-close[i]) / close[i] \n",
    "    if high_volatility >= threshold:\n",
    "        targets.append(0) \n",
    "    elif low_volatility <= -threshold:\n",
    "        targets.append(1) \n",
    "    else:\n",
    "        targets.append(2) # do not trade \n",
    "\n",
    "targets.append(None) \n",
    "\n",
    "chart_df['Targets'] = targets \n",
    "        \n",
    "chart_df.set_index(pd.DatetimeIndex(chart_df[\"datetime\"]), inplace=True)\n",
    "\n",
    "print(\"=== Feature Engineering ===\")\n",
    "chart_df['ebsw'] = chart_df.ta.ebsw(lookahead=False)\n",
    "chart_df['cmf'] = chart_df.ta.cmf(lookahead=False)\n",
    "chart_df['vwap'] = chart_df.ta.vwap(lookahead=False)  \n",
    "chart_df['vwap/open'] = chart_df['vwap'] / chart_df['open']\n",
    "chart_df['high/low'] = chart_df['high'] / chart_df['low'] \n",
    "chart_df['close/open'] = chart_df['close'] / chart_df['open']\n",
    "chart_df['high/open'] = chart_df['high'] / chart_df['open'] \n",
    "chart_df['low/open'] = chart_df['low'] / chart_df['open']\n",
    "\n",
    "# differencing \n",
    "for l in range(1,6): \n",
    "    for col in ['open','high','low','close','volume']: \n",
    "        val = chart_df[col].values \n",
    "        val_ret = [None for _ in range(l)] \n",
    "        for i in range(l, len(val)):\n",
    "            if val[i-l] == 0:\n",
    "                ret = 1\n",
    "            else:\n",
    "                ret = val[i] / val[i-l] \n",
    "            val_ret.append(ret) \n",
    "        chart_df['{}_change_{}'.format(col, l)] = val_ret   \n",
    "\n",
    "\n",
    "chart_df = chart_df.dropna()  \n",
    "chart_df = chart_df.drop(columns={'datetime', \n",
    "                                  'open', \n",
    "                                  'high', \n",
    "                                  'low', \n",
    "                                  'close', \n",
    "                                  'volume', \n",
    "                                  'vwap'}) \n",
    "\n",
    "chart_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9693, 37), (102, 37), (409, 37))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(chart_df.shape[0] * 0.95) \n",
    "train_df = chart_df.iloc[:train_size,:] \n",
    "\n",
    "val_size = int(chart_df.shape[0] * 0.01) \n",
    "val_df = chart_df.iloc[train_size:train_size+val_size,:] \n",
    "\n",
    "test_df = chart_df.iloc[train_size+val_size:,:] \n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [] \n",
    "for col in train_df.columns:\n",
    "    if col != 'Targets' and col != 'year':\n",
    "        input_columns.append(col)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9693, 35), (9693,), (102, 35), (102,), (409, 35), (409,))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df[input_columns].values  \n",
    "Y_train = train_df['Targets'].values\n",
    "\n",
    "X_val = val_df[input_columns].values \n",
    "Y_val = val_df['Targets'].values \n",
    "\n",
    "X_test = test_df[input_columns].values\n",
    "Y_test = test_df['Targets'].values\n",
    "\n",
    "\n",
    "\n",
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6463292658531706, 1: 0.9348958333333334, 2: 2.609854604200323}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight = \"balanced\", \n",
    "                                     classes = np.unique(Y_train), \n",
    "                                     y = Y_train)\n",
    "\n",
    "d = {0:class_weights[0], 1:class_weights[1], 2:class_weights[2]}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 1.17434 | val_0_logloss: 1.16684 | val_0_balanced_accuracy: 0.33453 |  0:00:00s\n",
      "epoch 1  | loss: 1.01385 | val_0_logloss: 1.10393 | val_0_balanced_accuracy: 0.33652 |  0:00:01s\n",
      "epoch 2  | loss: 0.88552 | val_0_logloss: 1.45336 | val_0_balanced_accuracy: 0.33333 |  0:00:02s\n",
      "epoch 3  | loss: 0.7456  | val_0_logloss: 1.83435 | val_0_balanced_accuracy: 0.33333 |  0:00:02s\n",
      "epoch 4  | loss: 0.71435 | val_0_logloss: 1.73198 | val_0_balanced_accuracy: 0.33333 |  0:00:03s\n",
      "epoch 5  | loss: 0.68889 | val_0_logloss: 1.74969 | val_0_balanced_accuracy: 0.33333 |  0:00:04s\n",
      "epoch 6  | loss: 0.67299 | val_0_logloss: 1.61855 | val_0_balanced_accuracy: 0.33333 |  0:00:04s\n",
      "epoch 7  | loss: 0.67935 | val_0_logloss: 1.43538 | val_0_balanced_accuracy: 0.33333 |  0:00:05s\n",
      "epoch 8  | loss: 0.6562  | val_0_logloss: 1.64176 | val_0_balanced_accuracy: 0.33333 |  0:00:06s\n",
      "epoch 9  | loss: 0.65319 | val_0_logloss: 1.37067 | val_0_balanced_accuracy: 0.33333 |  0:00:06s\n",
      "epoch 10 | loss: 0.64983 | val_0_logloss: 1.35616 | val_0_balanced_accuracy: 0.33333 |  0:00:07s\n",
      "epoch 11 | loss: 0.63436 | val_0_logloss: 1.45394 | val_0_balanced_accuracy: 0.33333 |  0:00:08s\n",
      "epoch 12 | loss: 0.64782 | val_0_logloss: 1.11273 | val_0_balanced_accuracy: 0.33333 |  0:00:08s\n",
      "epoch 13 | loss: 0.62589 | val_0_logloss: 1.31654 | val_0_balanced_accuracy: 0.33333 |  0:00:09s\n",
      "epoch 14 | loss: 0.62356 | val_0_logloss: 1.30594 | val_0_balanced_accuracy: 0.33333 |  0:00:10s\n",
      "epoch 15 | loss: 0.61998 | val_0_logloss: 1.11217 | val_0_balanced_accuracy: 0.33333 |  0:00:10s\n",
      "epoch 16 | loss: 0.61006 | val_0_logloss: 1.10456 | val_0_balanced_accuracy: 0.33333 |  0:00:11s\n",
      "epoch 17 | loss: 0.615   | val_0_logloss: 1.15052 | val_0_balanced_accuracy: 0.33333 |  0:00:12s\n",
      "epoch 18 | loss: 0.60017 | val_0_logloss: 1.11512 | val_0_balanced_accuracy: 0.33333 |  0:00:12s\n",
      "epoch 19 | loss: 0.62078 | val_0_logloss: 1.02442 | val_0_balanced_accuracy: 0.33333 |  0:00:13s\n",
      "epoch 20 | loss: 0.60055 | val_0_logloss: 1.08118 | val_0_balanced_accuracy: 0.33333 |  0:00:14s\n",
      "epoch 21 | loss: 0.61301 | val_0_logloss: 1.03672 | val_0_balanced_accuracy: 0.33333 |  0:00:14s\n",
      "epoch 22 | loss: 0.59696 | val_0_logloss: 1.03344 | val_0_balanced_accuracy: 0.33333 |  0:00:15s\n",
      "epoch 23 | loss: 0.59888 | val_0_logloss: 1.0882  | val_0_balanced_accuracy: 0.33333 |  0:00:16s\n",
      "epoch 24 | loss: 0.59489 | val_0_logloss: 1.0304  | val_0_balanced_accuracy: 0.33333 |  0:00:16s\n",
      "epoch 25 | loss: 0.6048  | val_0_logloss: 1.03075 | val_0_balanced_accuracy: 0.33333 |  0:00:17s\n",
      "epoch 26 | loss: 0.59608 | val_0_logloss: 1.02844 | val_0_balanced_accuracy: 0.33333 |  0:00:18s\n",
      "epoch 27 | loss: 0.59872 | val_0_logloss: 1.00204 | val_0_balanced_accuracy: 0.33333 |  0:00:18s\n",
      "epoch 28 | loss: 0.58837 | val_0_logloss: 0.98666 | val_0_balanced_accuracy: 0.33333 |  0:00:19s\n",
      "epoch 29 | loss: 0.59025 | val_0_logloss: 1.02062 | val_0_balanced_accuracy: 0.39181 |  0:00:20s\n",
      "epoch 30 | loss: 0.58374 | val_0_logloss: 1.06434 | val_0_balanced_accuracy: 0.35526 |  0:00:20s\n",
      "epoch 31 | loss: 0.5911  | val_0_logloss: 1.0192  | val_0_balanced_accuracy: 0.37972 |  0:00:21s\n",
      "epoch 32 | loss: 0.59743 | val_0_logloss: 1.02463 | val_0_balanced_accuracy: 0.36962 |  0:00:22s\n",
      "epoch 33 | loss: 0.59246 | val_0_logloss: 0.96921 | val_0_balanced_accuracy: 0.36829 |  0:00:22s\n",
      "epoch 34 | loss: 0.58618 | val_0_logloss: 0.9736  | val_0_balanced_accuracy: 0.41481 |  0:00:23s\n",
      "epoch 35 | loss: 0.58034 | val_0_logloss: 1.00303 | val_0_balanced_accuracy: 0.38876 |  0:00:23s\n",
      "epoch 36 | loss: 0.59072 | val_0_logloss: 0.92983 | val_0_balanced_accuracy: 0.44976 |  0:00:24s\n",
      "epoch 37 | loss: 0.57989 | val_0_logloss: 0.96986 | val_0_balanced_accuracy: 0.44351 |  0:00:25s\n",
      "epoch 38 | loss: 0.57424 | val_0_logloss: 0.93057 | val_0_balanced_accuracy: 0.48804 |  0:00:25s\n",
      "epoch 39 | loss: 0.59036 | val_0_logloss: 0.96861 | val_0_balanced_accuracy: 0.49747 |  0:00:26s\n",
      "epoch 40 | loss: 0.57485 | val_0_logloss: 0.93524 | val_0_balanced_accuracy: 0.45601 |  0:00:27s\n",
      "epoch 41 | loss: 0.57264 | val_0_logloss: 0.93363 | val_0_balanced_accuracy: 0.50572 |  0:00:27s\n",
      "epoch 42 | loss: 0.57304 | val_0_logloss: 0.94223 | val_0_balanced_accuracy: 0.5715  |  0:00:28s\n",
      "epoch 43 | loss: 0.57062 | val_0_logloss: 0.88691 | val_0_balanced_accuracy: 0.53416 |  0:00:29s\n",
      "epoch 44 | loss: 0.56759 | val_0_logloss: 0.94566 | val_0_balanced_accuracy: 0.58612 |  0:00:29s\n",
      "epoch 45 | loss: 0.57803 | val_0_logloss: 0.91316 | val_0_balanced_accuracy: 0.542   |  0:00:30s\n",
      "epoch 46 | loss: 0.56154 | val_0_logloss: 0.86309 | val_0_balanced_accuracy: 0.59569 |  0:00:30s\n",
      "epoch 47 | loss: 0.57171 | val_0_logloss: 0.8618  | val_0_balanced_accuracy: 0.54599 |  0:00:31s\n",
      "epoch 48 | loss: 0.56582 | val_0_logloss: 0.8786  | val_0_balanced_accuracy: 0.60513 |  0:00:32s\n",
      "epoch 49 | loss: 0.57054 | val_0_logloss: 0.88419 | val_0_balanced_accuracy: 0.53283 |  0:00:32s\n",
      "epoch 50 | loss: 0.57841 | val_0_logloss: 0.84546 | val_0_balanced_accuracy: 0.55781 |  0:00:33s\n",
      "epoch 51 | loss: 0.56802 | val_0_logloss: 0.83517 | val_0_balanced_accuracy: 0.62241 |  0:00:34s\n",
      "epoch 52 | loss: 0.58015 | val_0_logloss: 0.8435  | val_0_balanced_accuracy: 0.57549 |  0:00:34s\n",
      "epoch 53 | loss: 0.5761  | val_0_logloss: 0.85273 | val_0_balanced_accuracy: 0.58998 |  0:00:35s\n",
      "epoch 54 | loss: 0.57419 | val_0_logloss: 0.87369 | val_0_balanced_accuracy: 0.57935 |  0:00:36s\n",
      "epoch 55 | loss: 0.56342 | val_0_logloss: 0.81375 | val_0_balanced_accuracy: 0.61935 |  0:00:36s\n",
      "epoch 56 | loss: 0.58827 | val_0_logloss: 0.82979 | val_0_balanced_accuracy: 0.54027 |  0:00:37s\n",
      "epoch 57 | loss: 0.58216 | val_0_logloss: 0.839   | val_0_balanced_accuracy: 0.75266 |  0:00:38s\n",
      "epoch 58 | loss: 0.5729  | val_0_logloss: 0.74682 | val_0_balanced_accuracy: 0.55928 |  0:00:38s\n",
      "epoch 59 | loss: 0.57504 | val_0_logloss: 0.79063 | val_0_balanced_accuracy: 0.57815 |  0:00:39s\n",
      "epoch 60 | loss: 0.56868 | val_0_logloss: 0.76413 | val_0_balanced_accuracy: 0.55183 |  0:00:40s\n",
      "epoch 61 | loss: 0.5748  | val_0_logloss: 0.73277 | val_0_balanced_accuracy: 0.54599 |  0:00:40s\n",
      "epoch 62 | loss: 0.5643  | val_0_logloss: 0.76633 | val_0_balanced_accuracy: 0.61337 |  0:00:41s\n",
      "epoch 63 | loss: 0.56332 | val_0_logloss: 0.70299 | val_0_balanced_accuracy: 0.56366 |  0:00:41s\n",
      "epoch 64 | loss: 0.58035 | val_0_logloss: 0.74198 | val_0_balanced_accuracy: 0.59583 |  0:00:42s\n",
      "epoch 65 | loss: 0.55902 | val_0_logloss: 0.77958 | val_0_balanced_accuracy: 0.61483 |  0:00:43s\n",
      "epoch 66 | loss: 0.55622 | val_0_logloss: 0.75719 | val_0_balanced_accuracy: 0.57243 |  0:00:43s\n",
      "epoch 67 | loss: 0.56858 | val_0_logloss: 0.6983  | val_0_balanced_accuracy: 0.56366 |  0:00:44s\n",
      "epoch 68 | loss: 0.57292 | val_0_logloss: 0.72093 | val_0_balanced_accuracy: 0.53442 |  0:00:45s\n",
      "epoch 69 | loss: 0.56626 | val_0_logloss: 0.69748 | val_0_balanced_accuracy: 0.56805 |  0:00:45s\n",
      "epoch 70 | loss: 0.56063 | val_0_logloss: 0.74522 | val_0_balanced_accuracy: 0.59436 |  0:00:46s\n",
      "epoch 71 | loss: 0.56692 | val_0_logloss: 0.71475 | val_0_balanced_accuracy: 0.62214 |  0:00:47s\n",
      "epoch 72 | loss: 0.57456 | val_0_logloss: 0.71083 | val_0_balanced_accuracy: 0.60008 |  0:00:47s\n",
      "epoch 73 | loss: 0.56725 | val_0_logloss: 0.68863 | val_0_balanced_accuracy: 0.57243 |  0:00:48s\n",
      "epoch 74 | loss: 0.56811 | val_0_logloss: 0.68616 | val_0_balanced_accuracy: 0.56645 |  0:00:49s\n",
      "epoch 75 | loss: 0.56436 | val_0_logloss: 0.6958  | val_0_balanced_accuracy: 0.59277 |  0:00:49s\n",
      "epoch 76 | loss: 0.56656 | val_0_logloss: 0.64511 | val_0_balanced_accuracy: 0.59144 |  0:00:50s\n",
      "epoch 77 | loss: 0.55478 | val_0_logloss: 0.67044 | val_0_balanced_accuracy: 0.57243 |  0:00:51s\n",
      "epoch 78 | loss: 0.57218 | val_0_logloss: 0.67164 | val_0_balanced_accuracy: 0.6135  |  0:00:51s\n",
      "epoch 79 | loss: 0.55299 | val_0_logloss: 0.64959 | val_0_balanced_accuracy: 0.61935 |  0:00:52s\n",
      "epoch 80 | loss: 0.55925 | val_0_logloss: 0.6723  | val_0_balanced_accuracy: 0.58838 |  0:00:53s\n",
      "epoch 81 | loss: 0.56552 | val_0_logloss: 0.62226 | val_0_balanced_accuracy: 0.60752 |  0:00:53s\n",
      "epoch 82 | loss: 0.561   | val_0_logloss: 0.63518 | val_0_balanced_accuracy: 0.61789 |  0:00:54s\n",
      "epoch 83 | loss: 0.54132 | val_0_logloss: 0.60436 | val_0_balanced_accuracy: 0.64115 |  0:00:54s\n",
      "epoch 84 | loss: 0.5586  | val_0_logloss: 0.63495 | val_0_balanced_accuracy: 0.60752 |  0:00:55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85 | loss: 0.55796 | val_0_logloss: 0.63965 | val_0_balanced_accuracy: 0.66746 |  0:00:56s\n",
      "epoch 86 | loss: 0.57173 | val_0_logloss: 0.66597 | val_0_balanced_accuracy: 0.62201 |  0:00:56s\n",
      "epoch 87 | loss: 0.56514 | val_0_logloss: 0.6701  | val_0_balanced_accuracy: 0.65324 |  0:00:57s\n",
      "epoch 88 | loss: 0.55217 | val_0_logloss: 0.65034 | val_0_balanced_accuracy: 0.65152 |  0:00:58s\n",
      "epoch 89 | loss: 0.56539 | val_0_logloss: 0.65124 | val_0_balanced_accuracy: 0.58001 |  0:00:58s\n",
      "epoch 90 | loss: 0.55859 | val_0_logloss: 0.64246 | val_0_balanced_accuracy: 0.58161 |  0:00:59s\n",
      "epoch 91 | loss: 0.56154 | val_0_logloss: 0.62911 | val_0_balanced_accuracy: 0.60606 |  0:00:59s\n",
      "epoch 92 | loss: 0.56253 | val_0_logloss: 0.65059 | val_0_balanced_accuracy: 0.68939 |  0:01:00s\n",
      "epoch 93 | loss: 0.55983 | val_0_logloss: 0.68266 | val_0_balanced_accuracy: 0.71558 |  0:01:01s\n",
      "epoch 94 | loss: 0.54851 | val_0_logloss: 0.66027 | val_0_balanced_accuracy: 0.63131 |  0:01:01s\n",
      "epoch 95 | loss: 0.54927 | val_0_logloss: 0.63222 | val_0_balanced_accuracy: 0.62108 |  0:01:02s\n",
      "epoch 96 | loss: 0.54827 | val_0_logloss: 0.6416  | val_0_balanced_accuracy: 0.60354 |  0:01:03s\n",
      "epoch 97 | loss: 0.54748 | val_0_logloss: 0.67054 | val_0_balanced_accuracy: 0.68102 |  0:01:03s\n",
      "epoch 98 | loss: 0.55484 | val_0_logloss: 0.62415 | val_0_balanced_accuracy: 0.66746 |  0:01:04s\n",
      "epoch 99 | loss: 0.55891 | val_0_logloss: 0.61548 | val_0_balanced_accuracy: 0.61935 |  0:01:05s\n",
      "epoch 100| loss: 0.55266 | val_0_logloss: 0.6474  | val_0_balanced_accuracy: 0.59171 |  0:01:05s\n",
      "epoch 101| loss: 0.56147 | val_0_logloss: 0.66686 | val_0_balanced_accuracy: 0.63809 |  0:01:06s\n",
      "epoch 102| loss: 0.54784 | val_0_logloss: 0.624   | val_0_balanced_accuracy: 0.63118 |  0:01:07s\n",
      "epoch 103| loss: 0.54605 | val_0_logloss: 0.62016 | val_0_balanced_accuracy: 0.63118 |  0:01:07s\n",
      "epoch 104| loss: 0.56775 | val_0_logloss: 0.6537  | val_0_balanced_accuracy: 0.62959 |  0:01:08s\n",
      "epoch 105| loss: 0.56205 | val_0_logloss: 0.70775 | val_0_balanced_accuracy: 0.67185 |  0:01:08s\n",
      "epoch 106| loss: 0.57518 | val_0_logloss: 0.67455 | val_0_balanced_accuracy: 0.61789 |  0:01:09s\n",
      "epoch 107| loss: 0.56951 | val_0_logloss: 0.66314 | val_0_balanced_accuracy: 0.61789 |  0:01:10s\n",
      "epoch 108| loss: 0.56718 | val_0_logloss: 0.6637  | val_0_balanced_accuracy: 0.66919 |  0:01:10s\n",
      "epoch 109| loss: 0.56094 | val_0_logloss: 0.66186 | val_0_balanced_accuracy: 0.67929 |  0:01:11s\n",
      "epoch 110| loss: 0.56583 | val_0_logloss: 0.66474 | val_0_balanced_accuracy: 0.68102 |  0:01:12s\n",
      "epoch 111| loss: 0.55048 | val_0_logloss: 0.62885 | val_0_balanced_accuracy: 0.64726 |  0:01:12s\n",
      "epoch 112| loss: 0.56278 | val_0_logloss: 0.60857 | val_0_balanced_accuracy: 0.67172 |  0:01:13s\n",
      "epoch 113| loss: 0.57163 | val_0_logloss: 0.59899 | val_0_balanced_accuracy: 0.65723 |  0:01:14s\n",
      "epoch 114| loss: 0.56823 | val_0_logloss: 0.65165 | val_0_balanced_accuracy: 0.62374 |  0:01:14s\n",
      "epoch 115| loss: 0.56296 | val_0_logloss: 0.64089 | val_0_balanced_accuracy: 0.65736 |  0:01:15s\n",
      "epoch 116| loss: 0.54411 | val_0_logloss: 0.64635 | val_0_balanced_accuracy: 0.65736 |  0:01:16s\n",
      "epoch 117| loss: 0.574   | val_0_logloss: 0.64952 | val_0_balanced_accuracy: 0.60779 |  0:01:16s\n",
      "epoch 118| loss: 0.55334 | val_0_logloss: 0.62105 | val_0_balanced_accuracy: 0.63384 |  0:01:17s\n",
      "epoch 119| loss: 0.55304 | val_0_logloss: 0.64217 | val_0_balanced_accuracy: 0.63543 |  0:01:17s\n",
      "epoch 120| loss: 0.54822 | val_0_logloss: 0.68478 | val_0_balanced_accuracy: 0.60354 |  0:01:18s\n",
      "epoch 121| loss: 0.56827 | val_0_logloss: 0.64277 | val_0_balanced_accuracy: 0.64726 |  0:01:19s\n",
      "epoch 122| loss: 0.55434 | val_0_logloss: 0.6767  | val_0_balanced_accuracy: 0.62972 |  0:01:19s\n",
      "epoch 123| loss: 0.55251 | val_0_logloss: 0.65781 | val_0_balanced_accuracy: 0.61364 |  0:01:20s\n",
      "epoch 124| loss: 0.55273 | val_0_logloss: 0.64191 | val_0_balanced_accuracy: 0.66906 |  0:01:21s\n",
      "epoch 125| loss: 0.55063 | val_0_logloss: 0.65289 | val_0_balanced_accuracy: 0.58586 |  0:01:21s\n",
      "epoch 126| loss: 0.5505  | val_0_logloss: 0.66053 | val_0_balanced_accuracy: 0.60021 |  0:01:22s\n",
      "epoch 127| loss: 0.54747 | val_0_logloss: 0.62051 | val_0_balanced_accuracy: 0.62959 |  0:01:23s\n",
      "epoch 128| loss: 0.57177 | val_0_logloss: 0.61415 | val_0_balanced_accuracy: 0.60181 |  0:01:23s\n",
      "epoch 129| loss: 0.56296 | val_0_logloss: 0.63534 | val_0_balanced_accuracy: 0.64141 |  0:01:24s\n",
      "epoch 130| loss: 0.54513 | val_0_logloss: 0.61622 | val_0_balanced_accuracy: 0.66321 |  0:01:25s\n",
      "epoch 131| loss: 0.53958 | val_0_logloss: 0.7059  | val_0_balanced_accuracy: 0.62547 |  0:01:25s\n",
      "epoch 132| loss: 0.55674 | val_0_logloss: 0.64175 | val_0_balanced_accuracy: 0.64726 |  0:01:26s\n",
      "epoch 133| loss: 0.55507 | val_0_logloss: 0.65119 | val_0_balanced_accuracy: 0.62374 |  0:01:27s\n",
      "epoch 134| loss: 0.55343 | val_0_logloss: 0.65906 | val_0_balanced_accuracy: 0.59755 |  0:01:27s\n",
      "epoch 135| loss: 0.55964 | val_0_logloss: 0.63165 | val_0_balanced_accuracy: 0.64301 |  0:01:28s\n",
      "epoch 136| loss: 0.55642 | val_0_logloss: 0.62835 | val_0_balanced_accuracy: 0.66321 |  0:01:28s\n",
      "epoch 137| loss: 0.55454 | val_0_logloss: 0.6928  | val_0_balanced_accuracy: 0.60606 |  0:01:29s\n",
      "epoch 138| loss: 0.55166 | val_0_logloss: 0.60674 | val_0_balanced_accuracy: 0.59915 |  0:01:30s\n",
      "epoch 139| loss: 0.55422 | val_0_logloss: 0.64801 | val_0_balanced_accuracy: 0.64141 |  0:01:30s\n",
      "epoch 140| loss: 0.55785 | val_0_logloss: 0.61026 | val_0_balanced_accuracy: 0.67331 |  0:01:31s\n",
      "epoch 141| loss: 0.54448 | val_0_logloss: 0.68156 | val_0_balanced_accuracy: 0.65152 |  0:01:32s\n",
      "epoch 142| loss: 0.55766 | val_0_logloss: 0.63853 | val_0_balanced_accuracy: 0.61364 |  0:01:32s\n",
      "epoch 143| loss: 0.53979 | val_0_logloss: 0.67655 | val_0_balanced_accuracy: 0.65577 |  0:01:33s\n",
      "epoch 144| loss: 0.541   | val_0_logloss: 0.646   | val_0_balanced_accuracy: 0.69963 |  0:01:34s\n",
      "epoch 145| loss: 0.53895 | val_0_logloss: 0.68179 | val_0_balanced_accuracy: 0.65736 |  0:01:34s\n",
      "epoch 146| loss: 0.55214 | val_0_logloss: 0.66163 | val_0_balanced_accuracy: 0.65577 |  0:01:35s\n",
      "epoch 147| loss: 0.54497 | val_0_logloss: 0.67532 | val_0_balanced_accuracy: 0.65577 |  0:01:36s\n",
      "epoch 148| loss: 0.55043 | val_0_logloss: 0.7133  | val_0_balanced_accuracy: 0.63557 |  0:01:36s\n",
      "epoch 149| loss: 0.54891 | val_0_logloss: 0.67447 | val_0_balanced_accuracy: 0.60779 |  0:01:37s\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 57 and best_val_0_balanced_accuracy = 0.75266\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetClassifier() \n",
    "\n",
    "clf.fit(X_train, \n",
    "        Y_train, \n",
    "        eval_set=[(X_val, Y_val)], \n",
    "        eval_metric=['logloss', 'balanced_accuracy'],\n",
    "        weights = d, \n",
    "        max_epochs=150,\n",
    "        patience=150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.325183374083124"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "pred = clf.predict(X_test)\n",
    "for i in range(len(pred)):\n",
    "    if Y_test[i] == float(pred[i]):\n",
    "        cnt += 1 \n",
    "        \n",
    "cnt / len(pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6147525076552174\n",
      "0.6332518337408313\n",
      "0.6702039196118003\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(Y_test, pred, average='macro'))\n",
    "print(f1_score(Y_test, pred, average='micro'))\n",
    "print(f1_score(Y_test, pred, average='weighted')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
